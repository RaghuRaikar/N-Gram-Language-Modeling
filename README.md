ğŸ“– N-Gram Language Modeling with Additive Smoothing & Interpolation ğŸš€
======================================================================

### ğŸ” Overview

This project implements **n-gram language models** (unigram, bigram, and trigram) with **additive smoothing** and **linear interpolation** to analyze and generate probabilistic language models. The goal is to estimate the likelihood of word sequences in natural language and evaluate model performance using **perplexity scores**.

The program is tested using the **One Billion Word Benchmark dataset**, but it can be used for any large-scale text corpus.

* * * * *

ğŸ¯ Features
-----------

âœ… Implements **unigram, bigram, and trigram** language models\
âœ… Supports **additive smoothing** (Laplace smoothing) with tunable Î± values\
âœ… Implements **linear interpolation smoothing** with customizable Î» values\
âœ… **Computes perplexity** on training, development, and test sets\
âœ… Processes large-scale text datasets efficiently\
âœ… Allows evaluation and comparison of different smoothing techniques

* * * * *

ğŸ›  How It Works
---------------

The program reads a large dataset of tokenized text and processes it as follows:

1ï¸âƒ£ **Tokenization:** Reads input text and converts it into a structured n-gram format\
2ï¸âƒ£ **Model Training:** Builds **unigram, bigram, and trigram** probability distributions\
3ï¸âƒ£ **Smoothing Techniques:** Applies:

-   **Additive smoothing** (Laplace smoothing) to handle unseen words
-   **Linear interpolation smoothing** to combine n-gram probabilities\
    4ï¸âƒ£ **Perplexity Calculation:** Evaluates how well the model predicts unseen text\
    5ï¸âƒ£ **Evaluation & Comparison:** Reports results on train, dev, and test sets

* * * * *

ğŸ“¥ Inputs & Outputs
-------------------

### ğŸ“Œ Input Files:

-   **Train Dataset:** `1b_benchmark.train.tokens` (Large-scale corpus for model training)
-   **Dev Dataset:** `1b_benchmark.dev.tokens` (Used to tune hyperparameters)
-   **Test Dataset:** `1b_benchmark.test.tokens` (Evaluates final model performance)

### ğŸ“Œ Output:

-   **Perplexity Scores** for different n-gram models and smoothing techniques
-   **Best performing hyperparameters** based on development set results
-   **Comparison between raw and smoothed models**

* * * * *

ğŸ“Š Performance & Results
------------------------

The model was evaluated on the One Billion Word dataset, and the **perplexity scores** were as follows:

### ğŸ“Œ Raw N-Gram Model Perplexities:

| Model | Train Perplexity | Dev Perplexity | Test Perplexity |
| --- | --- | --- | --- |
| Unigram | 976.5 | 2406.3 | 2380.4 |
| Bigram | 77.1 | 18704.8 | 18202.4 |
| Trigram | 7.9 | 11645330.1 | 11214441.99 |

### ğŸ“Œ With Additive Smoothing (Î± = 1):

| Model | Train Perplexity | Dev Perplexity |
| --- | --- | --- |
| Unigram | 977.51 | 2410.16 |
| Bigram | 1442.31 | 114658.51 |
| Trigram | 6244.42 | 66381731.91 |

### ğŸ“Œ With Linear Interpolation (Î»1 = 0.1, Î»2 = 0.3, Î»3 = 0.6):

-   **Perplexity:** ~48.1 (Evaluation Set)

ğŸ”¹ **Key Findings:**

-   Without smoothing, **higher-order n-grams performed well on training data** but suffered on unseen data due to sparsity.
-   **Additive smoothing improved generalization** but introduced bias.
-   **Linear interpolation offered the best balance**, reducing overfitting while maintaining predictive power.

* * * * *

ğŸš€ Installation & Usage
-----------------------

### ğŸ“¥ 1. Clone the Repository

`git clone https://github.com/your-repo/N-Gram-Language-Modeling.git`  
`cd N-Gram-Language-Modeling`

### ğŸ— 2. Install Dependencies

Ensure you have Python installed, then install required packages:

`pip install -r requirements.txt`

### â–¶ï¸ 3. Run the Program

`python main.py --train 1b_benchmark.train.tokens --dev 1b_benchmark.dev.tokens --test 1b_benchmark.test.tokens --smoothing additive --alpha 1`  

ğŸ”¹ Modify `--smoothing` and `--alpha` to test different configurations.


ğŸ“œ Purpose & Applications
-------------------------

This program is **not just an assignment**---it has **real-world applications** in:

ğŸ”¹ **Speech Recognition:** Predicting the next word in voice input\
ğŸ”¹ **Text Generation:** Powering chatbots & AI writers\
ğŸ”¹ **Spelling & Grammar Correction:** Ranking candidate words based on likelihood\
ğŸ”¹ **Machine Translation:** Improving fluency in automated translations

ğŸ”¹ **Why This Matters?**

-   Helps machines understand natural language better
-   Reduces **word prediction errors** in AI systems
-   Improves **language-based AI applications**

* * * * *

ğŸ† Future Enhancements
----------------------

ğŸ’¡ Improve performance with **Kneser-Ney Smoothing**\
ğŸ’¡ Implement **neural-based language models (e.g., LSTMs, Transformers)**\
ğŸ’¡ Support **larger-scale datasets** beyond One Billion Word Benchmark

* * * * *

ğŸ”¥ **A powerful tool for language modeling---fast, efficient, and ready for real-world NLP tasks!** ğŸš€
